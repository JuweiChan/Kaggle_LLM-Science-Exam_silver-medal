{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 将数据转成embeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "base_dir = \"./input\"\n",
    "# \n",
    "input_csv_path = f\"{base_dir}/chris-60k/raw_60k.csv\"\n",
    "output1_pq_path = f\"{base_dir}/chris-60k/60k.parquet\"\n",
    "\n",
    "\n",
    "sub_df = pd.read_csv(input_csv_path)\n",
    "\n",
    "# 使用fillna函数填充DataFrame中的空值（NaN）。这里是将A到E这五列中的空值替换为字符串''\n",
    "sub_df['A'] = sub_df['A'].fillna('')\n",
    "sub_df['B'] = sub_df['B'].fillna('')\n",
    "sub_df['C'] = sub_df['C'].fillna('')\n",
    "sub_df['D'] = sub_df['D'].fillna('')\n",
    "sub_df['E'] = sub_df['E'].fillna('')\n",
    "\n",
    "# 创建一个新的列'all_text'，这个列是将'prompt', 'A', 'B', 'C', 'D', 'E'这些列的值连接起来\n",
    "sub_df['all_text'] = sub_df.apply(lambda x: \" \".join([x['prompt'], x['A'], x['B'], x['C'], x['D'], x['E']]), axis=1)\n",
    "print(f\"sub_df.shape: {sub_df.shape}\")\n",
    "sub_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "SIM_MODEL = 'BAAI/bge-large-en-v1.5'\n",
    "model = SentenceTransformer(SIM_MODEL, device='cuda')\n",
    "model = model.half() # use fp16\n",
    "\n",
    "embeds = []\n",
    "for all_text in tqdm(sub_df['all_text'].tolist()):\n",
    "    # 使用模型对文本进行编码，并将结果添加到embeds列表中\n",
    "    embeds.append(model.encode(all_text, show_progress_bar=False))\n",
    "embeds = np.array(embeds)\n",
    "print(f\"{embeds.shape=}\")\n",
    "sub_df[\"embeds\"] = embeds.tolist()\n",
    "sub_df.head()\n",
    "\n",
    "sub_df.to_parquet(output1_pq_path, engine='pyarrow')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 将每一篇wiki的全文 转为 embeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import faiss\n",
    "import heapq\n",
    "import pickle\n",
    "import gc\n",
    "import time \n",
    "from tqdm import tqdm\n",
    "\n",
    "from utils import get_timediff\n",
    "from datasets import load_dataset, load_from_disk\n",
    "\n",
    "\n",
    "base_dir = \"./input\"\n",
    "paraphs_parsed_dataset = load_from_disk(f\"{base_dir}/wiki-270k\")\n",
    "context_df = paraphs_parsed_dataset.to_pandas()\n",
    "context_df['title'] = context_df['title'].astype(str)\n",
    "context_df['section'] = context_df['section'].astype(str)\n",
    "context_df['text'] = context_df['text'].astype(str)\n",
    "# 将'标题'、'章节'、'文本'字段合并为新的字段'all_text'\n",
    "context_df['all_text'] = context_df['title'] + ' ' + context_df['section'] + ' ' + context_df['text']\n",
    "context_df.to_parquet(f\"{base_dir}/wiki-270k/wiki-270k-sentences.parquet\", engine='pyarrow')\n",
    "context_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "SIM_MODEL = 'BAAI/bge-large-en-v1.5'\n",
    "model = SentenceTransformer(SIM_MODEL, device='cuda')\n",
    "model = model.half()\n",
    "\n",
    "# 使用模型对'all_text'字段进行编码，得到嵌入向量\n",
    "context_embeds = model.encode(\n",
    "    context_df.all_text.values, \n",
    "    batch_size=128, \n",
    "    show_progress_bar=True, \n",
    "    convert_to_tensor=True,\n",
    "    # normalize_embeddings=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_embeds = context_embeds.detach().cpu().numpy()\n",
    "_ = gc.collect()\n",
    "context_embeds = context_embeds.astype(np.float32)\n",
    "_ = gc.collect()\n",
    "context_embeds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "from faiss import write_index\n",
    "\n",
    "# 创建一个FAISS索引\n",
    "dimension = context_embeds.shape[1]  # 获取嵌入向量的维度\n",
    "index = faiss.IndexFlatL2(dimension)  # 在这个例子中，我们使用L2距离的平面索引\n",
    "\n",
    "# 将嵌入向量添加到索引中\n",
    "index.add(context_embeds)\n",
    "\n",
    "# 将索引保存到磁盘\n",
    "write_index(index, f\"{base_dir}/wiki-270k/wiki-270k.index\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 为 题目embeds 匹配最相似的n个wiki段落"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入所需的库\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import faiss\n",
    "import heapq\n",
    "import pickle\n",
    "import gc\n",
    "import time \n",
    "from tqdm import tqdm\n",
    "import faiss\n",
    "from faiss import write_index, read_index\n",
    "import ctypes\n",
    "libc = ctypes.CDLL(\"libc.so.6\")\n",
    "\n",
    "from utils import get_timediff  # 导入自定义的get_timediff函数\n",
    "\n",
    "# 定义路径\n",
    "base_dir = \"./input\"\n",
    "context_path = f\"{base_dir}/wiki-270k/wiki-270k-sentences.parquet\"  # 定义wiki文本的路径\n",
    "context_index_path = f\"{base_dir}/wiki-270k/wiki-270k.index\"  # 定义wiki文本的索引路径\n",
    "train_pq_path = f\"{base_dir}/chris-60k/60k.parquet\"  # 定义训练数据的路径\n",
    "\n",
    "# 加载训练数据的embeddings\n",
    "train_df = pd.read_parquet(train_pq_path)  # 使用pandas的read_parquet函数读取训练数据\n",
    "print(f\"train_df.shape: {train_df.shape}\")  # 打印训练数据的形状\n",
    "train_emb = np.stack(train_df[\"embeds\"]).astype(np.float32)  # 将训练数据的embeddings堆叠成一个numpy数组，并转换为float32类型\n",
    "print(f\"train_emb.shape: {train_emb.shape}\")  # 打印训练数据embeddings的形状"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_ARTICLES = 5  # 定义每个问题需要找到的最相似的wiki文章数量\n",
    "context_index = read_index(context_index_path)  # 使用faiss的read_index函数加载wiki文本的索引\n",
    "print(f\"{context_index.ntotal=}\")  # 打印索引中的总文本数量\n",
    "\n",
    "print(\"Searching...\")  # 打印搜索开始的提示信息\n",
    "# 使用faiss的search函数在wiki文本中搜索每个训练数据的embeddings最相似的NUM_ARTICLES篇文章\n",
    "score, all_train_wiki_indices = context_index.search(train_emb, NUM_ARTICLES) \n",
    "all_train_wiki_indices.shape  # 打印最相似的wiki文章的索引的形状"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_df = pd.read_parquet(context_path, columns=[\"title\", \"all_text\"])  # 加载wiki文本，仅加载\"title\"和\"all_text\"两列\n",
    "\n",
    "all_train_texts = []  # 定义一个空列表用于存储所有训练数据的文本\n",
    "\n",
    "# 遍历所有训练数据的wiki文章索引\n",
    "for train_index, wiki_indices in enumerate(all_train_wiki_indices):\n",
    "    texts = context_df.iloc[wiki_indices].all_text.values  # 获取每个训练数据的最相似的wiki文章的文本\n",
    "    texts = texts.tolist()  # 将numpy数组转换为列表\n",
    "    all_train_texts.append(texts)  # 将文本添加到all_train_texts列表中\n",
    "\n",
    "train_df[\"context\"] = all_train_texts  # 将all_train_texts列表添加到训练数据的\"context\"列中\n",
    "\n",
    "train_df.to_parquet(train_pq_path, engine='pyarrow') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
