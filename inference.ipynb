{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-06T07:11:49.779547Z","iopub.status.busy":"2023-09-06T07:11:49.778981Z","iopub.status.idle":"2023-09-06T07:11:49.789979Z","shell.execute_reply":"2023-09-06T07:11:49.788948Z","shell.execute_reply.started":"2023-09-06T07:11:49.779515Z"},"trusted":true},"outputs":[],"source":["model_name = \"/kaggle/input/llama2-qlora\""]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-output":true,"execution":{"iopub.execute_input":"2023-09-06T07:11:49.792794Z","iopub.status.busy":"2023-09-06T07:11:49.792493Z","iopub.status.idle":"2023-09-06T07:14:29.822813Z","shell.execute_reply":"2023-09-06T07:14:29.821468Z","shell.execute_reply.started":"2023-09-06T07:11:49.792771Z"},"trusted":true},"outputs":[],"source":["import sys\n","sys.path.append(\"/kaggle/input/sentence-transformers-222/sentence-transformers\")\n","!pip -q install /kaggle/input/faiss-gpu-173-python310/faiss_gpu-1.7.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n","!pip -q install /kaggle/input/transformers431/transformers-4.31.0-py3-none-any.whl\n","!pip -q install /kaggle/input/bitsandbytes-0410/bitsandbytes-0.41.0-py3-none-any.whl\n","!pip -q install /kaggle/input/llm-lib/peft-0.5.0-py3-none-any.whl\n","!pip -q install /kaggle/input/llm-lib/accelerate-0.21.0-py3-none-any.whl"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-06T07:14:29.827581Z","iopub.status.busy":"2023-09-06T07:14:29.827269Z","iopub.status.idle":"2023-09-06T07:14:29.840312Z","shell.execute_reply":"2023-09-06T07:14:29.839336Z","shell.execute_reply.started":"2023-09-06T07:14:29.827553Z"},"trusted":true},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import ctypes\n","import pandas as pd\n","import numpy as np\n","import glob\n","import faiss  # Facebook AI Similarity Search，Facebook开源的一个高效相似度搜索库\n","from joblib import Parallel, delayed\n","import heapq\n","import pickle\n","import gc  # Python的垃圾回收模块\n","import sys\n","from tqdm import tqdm  # 用于显示进度条\n","from faiss import write_index, read_index\n","\n","sys.path.append(\"/kaggle/input/sentence-transformers-222/sentence-transformers\")\n","libc = ctypes.CDLL(\"libc.so.6\")  # 加载C库\n","\n","def get_context():\n","    # 读取sub_df\n","    sub_df = pd.read_csv(\"/kaggle/input/kaggle-llm-science-exam/test.csv\")\n","    if len(sub_df) == 200:  # 如果sub_df的长度为200，则将sub_df替换为train.csv中的数据\n","        sub_df = pd.read_csv(\"/kaggle/input/kaggle-llm-science-exam/train.csv\")\n","        print(\"read train.csv\")\n","    # 将prompt, A, B, C, D, E字段的内容连接起来，作为all_text字段的内容\n","    sub_df['all_text'] = sub_df.apply(lambda x: \" \".join([x['prompt'], x['A'], x['B'], x['C'], x['D'], x['E']]), axis=1)\n","    print(f\"sub_df.shape: {sub_df.shape}\")\n","\n","    # 创建 embed 模型\n","    from sentence_transformers import SentenceTransformer\n","    SIM_MODEL = f'/kaggle/input/bge-large-en-v1-5/bge-large-en-v1.5'\n","    model = SentenceTransformer(SIM_MODEL, device='cuda')  # 使用SentenceTransformer模型，设备为cuda\n","    model = model.half()  # 将模型的参数转为半精度浮点数（float16）\n","\n","    # 对sub_df进行embedding\n","    embeds = []\n","    for all_text in sub_df['all_text'].tolist():\n","        embeds.append(model.encode(all_text, show_progress_bar=False))  # 对all_text字段的内容进行embedding\n","    embeds = np.array(embeds)\n","    print(f\"{embeds.shape=}\")\n","    sub_df[\"embeds\"] = embeds.tolist()  # 将embedding的结果添加到sub_df的embeds字段中\n","\n","    sub_emb = np.stack(sub_df[\"embeds\"]).astype(np.float32)\n","    print(f\"sub_emb.shape: {sub_emb.shape}\")\n","\n","    # 文件来自于 data_preprocess.ipynb\n","    context_path = f\"/kaggle/input/wiki-270k/wiki-270k-sentences.parquet\"\n","    context_index_path = f\"/kaggle/input/wiki-270k/wiki-270k.index\"\n","\n","    # 为每个题目加上寻找最相似的n个context段落\n","    NUM_ARTICLES = 5\n","    context_index = read_index(context_index_path)  # 加载wiki270k的index\n","    print(f\"{context_index.ntotal=}\")\n","\n","    print(\"Searching...\")\n","    # 使用faiss进行最近邻搜索，找到与每个题目最相似的5篇wiki文章\n","    score, all_test_wiki_indices = context_index.search(sub_emb, NUM_ARTICLES) \n","\n","    context_df = pd.read_parquet(context_path, columns=[\"title\", \"all_text\"])  # 读取wiki文章的标题和内容\n","    all_test_texts = []\n","\n","    for wiki_indices in all_test_wiki_indices:\n","        texts = context_df.iloc[wiki_indices].all_text.values  # 获取最相似的文章的内容\n","        all_test_texts.append(texts.tolist())\n","\n","    sub_df[\"context\"] = all_test_texts  # 将最相似的文章的内容添加到sub_df的context字段中\n","    gc.collect()  # 执行垃圾回收\n","\n","    # 保存 sub_df 文件\n","    sub_df.to_parquet(f\"/kaggle/working/all_data.parquet\", engine='pyarrow')\n","\n","\n","get_context()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-06T07:27:27.764630Z","iopub.status.busy":"2023-09-06T07:27:27.763886Z","iopub.status.idle":"2023-09-06T07:27:27.774684Z","shell.execute_reply":"2023-09-06T07:27:27.773583Z","shell.execute_reply.started":"2023-09-06T07:27:27.764592Z"},"trusted":true},"outputs":[],"source":["import psutil\n","\n","# 获取虚拟内存的统计信息\n","memory_info = psutil.virtual_memory()\n","\n","# 打印内存占用情况\n","print(f\"总内存：{memory_info.total / (1024**2):.2f} MB\")\n","print(f\"已使用内存：{memory_info.used / (1024**2):.2f} MB\")\n","print(f\"空闲内存：{memory_info.available / (1024**2):.2f} MB\")\n","print(f\"内存使用百分比：{memory_info.percent}%\")"]},{"cell_type":"markdown","metadata":{},"source":["# 大模型"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2023-09-06T07:36:00.723955Z","iopub.status.busy":"2023-09-06T07:36:00.723520Z","iopub.status.idle":"2023-09-06T07:36:13.276937Z","shell.execute_reply":"2023-09-06T07:36:13.276044Z","shell.execute_reply.started":"2023-09-06T07:36:00.723919Z"},"trusted":true},"outputs":[],"source":["import sys\n","import gc\n","import os\n","import numpy as np\n","import pandas as pd\n","from tqdm import tqdm\n","import torch\n","from torch import nn\n","from transformers import LlamaTokenizer, AutoModelForCausalLM, AutoTokenizer\n","import transformers\n","from transformers import BitsAndBytesConfig\n","from peft import AutoPeftModelForCausalLM\n","\n","transformers.__version__"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"execution":{"iopub.execute_input":"2023-09-06T08:06:29.200692Z","iopub.status.busy":"2023-09-06T08:06:29.200280Z","iopub.status.idle":"2023-09-06T08:06:29.531923Z","shell.execute_reply":"2023-09-06T08:06:29.530844Z","shell.execute_reply.started":"2023-09-06T08:06:29.200660Z"},"jupyter":{"outputs_hidden":true},"trusted":true},"outputs":[],"source":["# 导入AutoTokenizer，这是HuggingFace库中的一个类，它可以自动地从预训练模型中加载对应的tokenizer\n","tokenizer = AutoTokenizer.from_pretrained(\n","    model_name,  # 预训练模型的名称\n","    use_fast=True,  # 使用快速tokenizer\n","    trust_remote_code=True,  # 信任远程代码，如果模型包含自定义的tokenizer代码，则执行该代码\n","    truncation_side=\"left\",  # 如果文本超过模型的最大长度，那么从文本的左边开始截断\n",")\n","\n","# 定义一个函数，该函数用于格式化数据集中的一行数据\n","def format_instruction(row, mode):\n","    # 初始化一个空字符串\n","    text = \"\"\n","    # 在字符串中添加一些介绍性的文本\n","    text += \"The following are multiple choice questions (with answers) that include context\\n\"\n","    text += \"\\n\"\n","    text += \"Context:\\n\"\n","    # 从数据集的一行数据中提取出前五个上下文，并添加到字符串中\n","    for i in range(5):\n","        text += f\"{row['context'][i]}\\n\"\n","        if i != 4:\n","            text += \"###\\n\"\n","    text += \"\\n\"\n","    text += \"Question:\\n\"\n","    # 从数据集的一行数据中提取出问题，并添加到字符串中\n","    text += f\"{row['prompt']}\\n\"\n","    text += \"\\n\"\n","    text += \"Options:\\n\"\n","    # 从数据集的一行数据中提取出选项，并添加到字符串中\n","    text += f\"A: {row['A']}\\n\"\n","    text += f\"B: {row['B']}\\n\"\n","    text += f\"C: {row['C']}\\n\"\n","    text += f\"D: {row['D']}\\n\"\n","    text += f\"E: {row['E']}\\n\"\n","    text += \"\\n\"\n","    text += \"Answer: \"\n","\n","    # 如果是训练模式，那么还需要从数据集的一行数据中提取出答案，并添加到字符串中\n","    if mode == \"train\":\n","        text += f\"{row['answer']}\"\n","    return text  # 返回格式化后的字符串\n","\n","# 使用pandas库中的read_parquet函数，从.parquet文件中读取数据，返回一个DataFrame对象\n","df = pd.read_parquet(f\"/kaggle/working/all_data.parquet\")\n","# 对DataFrame的每一行应用format_instruction函数，将返回的字符串添加到新的一列中\n","df['instruction'] = df.apply(lambda row: format_instruction(row, \"test\"), axis=1)\n","\n","# 打印DataFrame的第一行的instruction列的值\n","print(df.iloc[0].instruction)\n"]},{"cell_type":"markdown","metadata":{},"source":["## 模型"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# 使用预训练模型创建一个自动Peft模型，用于因果语言模型（Causal Language Modeling）\n","# 因果语言模型是一种自然语言处理模型，它可以预测给定前文的情况下下一个词是什么\n","model = AutoPeftModelForCausalLM.from_pretrained(\n","    model_name,  # 预训练模型的名称，这个名称通常是预训练模型在Hugging Face Model Hub上的标识符\n","\n","    # \"device_map\"参数用于指定模型运行的设备。\"auto\"表示自动选择最佳设备，如果有GPU，则优先选择GPU，否则选择CPU\n","    device_map=\"auto\",  \n","   \n","    # \"low_cpu_mem_usage\"参数设置为True，表示在CPU内存使用方面进行优化，尽可能减少CPU内存的使用\n","    low_cpu_mem_usage=True,\n","    \n","    # \"torch_dtype\"参数指定了模型中张量的数据类型。在这里，我们使用的是半精度浮点数（float16），\n","    torch_dtype=torch.float16,\n","    \n","    # \"load_in_4bit\"参数如果设置为True，模型将以4位的精度加载，这可以进一步减少内存使用和计算时间，但可能会降低模型的精度\n","    # load_in_4bit=True,\n","    )\n","\n","model = model.eval()\n","\n","# 将tokenizer的pad_token（填充符）设置为eos_token（句子结束符）。\n","tokenizer.pad_token = tokenizer.eos_token\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import psutil\n","\n","# 获取虚拟内存的统计信息\n","memory_info = psutil.virtual_memory()\n","\n","# 打印内存占用情况\n","print(f\"总内存：{memory_info.total / (1024**2):.2f} MB\")\n","print(f\"已使用内存：{memory_info.used / (1024**2):.2f} MB\")\n","print(f\"空闲内存：{memory_info.available / (1024**2):.2f} MB\")\n","print(f\"内存使用百分比：{memory_info.percent}%\")"]},{"cell_type":"markdown","metadata":{},"source":["## 推理"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["preds = []  # 初始化一个空列表，用于存储预测结果\n","\n","# 对数据框 df 的每一行进行迭代处理\n","for _, row in tqdm(df.iterrows(), total=len(df)):\n","    # 使用 tokenizer 处理每一行中的 'instruction' 列的文本\n","    # return_tensors=\"pt\" 表示返回的是 PyTorch 张量\n","    # truncation=True 表示如果文本超过模型的最大长度，就截断它\n","    # max_length=2048 是设置的最大长度，如果文本的长度超过这个值，就会被截断\n","    inputs = tokenizer(\n","        row['instruction'], \n","        return_tensors=\"pt\",\n","        truncation=True, \n","        max_length=2048, \n","    ).to(f\"cuda:{model.device.index}\")\n","\n","    # 不计算梯度，以节省计算资源，因为这里只是进行推理，不需要更新模型的参数\n","    with torch.no_grad():\n","        # 使用模型处理输入，并获取输出的 logits\n","        # logits 是模型最后一层的输出，通常用于计算概率分布\n","        output = model(\n","            input_ids=inputs[\"input_ids\"], \n","            attention_mask=inputs[\"attention_mask\"]\n","            ).logits\n","\n","    # 找出 attention_mask 中非零元素的索引，因为 attention_mask 中的非零元素对应的是输入的有效 token\n","    non_zero_indices = torch.nonzero(inputs['attention_mask'])\n","    # 获取最后一个有效 token 的索引\n","    last_one_index = non_zero_indices[-1][1].item()\n","    # 获取最后一个有效 token 对应的 logits\n","    first_token_probs = output[0][last_one_index]\n","\n","    # 对每一个选项（' A'，' B'，' C'，' D'，' E'）进行处理\n","    # tokenizer(option).input_ids[-1] 是获取选项对应的 id\n","    # first_token_probs[id] 是获取该 id 对应的 logits\n","    # 然后将 logits 和选项一起作为一个元组存储在 options_list 中\n","    options_list = [\n","        (first_token_probs[tokenizer(' A').input_ids[-1]], 'A'),\n","        (first_token_probs[tokenizer(' B').input_ids[-1]], 'B'),\n","        (first_token_probs[tokenizer(' C').input_ids[-1]], 'C'),\n","        (first_token_probs[tokenizer(' D').input_ids[-1]], 'D'),\n","        (first_token_probs[tokenizer(' E').input_ids[-1]], 'E'),\n","    ]\n","    # 对 options_list 进行排序，排序的依据是 logits，也就是每个选项的概率\n","    options_list = sorted(options_list, reverse=True)\n","    \n","    # 初始化一个空列表，用于存储前三个最可能的选项\n","    pred = []\n","    for i in range(3):\n","        # 添加最可能的选项到 pred 中\n","        pred.append(options_list[i][1])\n","    # 将 pred 中的选项用空格连接成一个字符串\n","    pred = ' '.join(pred)\n","    # 将预测结果添加到 preds 中\n","    preds.append(pred)\n","    \n","# 将 preds 添加到 df 的 'prediction' 列中\n","df['prediction'] = preds\n","\n","# 将 df 的 'id' 列和 'prediction' 列保存到 'submission.csv' 文件中\n","df[[\"id\", \"prediction\"]].to_csv('submission.csv', index=False)"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":4}
